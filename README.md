# **Variant Classification and Interpretability of SNPs using  Custom CNN-LSTM Architecture with Custom SmoothGrad on 1D Genomic Sequences**
# Introduction
Single Nucleotide Polymorphisms (SNPs) are the most common type of genetic variation among individuals and play a crucial role in disease susceptibility, drug response, and personalized medicine. Accurate classification of SNPs as benign or pathogenic is essential for genomic diagnostics and therapeutic decision-making.

In this project, I have made a custom deep learning architecture that combines Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks to classify SNP variants. By leveraging dual input channels—one for reference sequences and another for alternate sequences, I aim to extract both local sequence motifs and long-range dependencies.

# Dataset
The dataset used for this project was obtained from **ClinVar**, a publicly available resource maintained by the **National Center for Biotechnology Information (NCBI)**. ClinVar aggregates information about genomic variations and their relationship to human health, including clinical significance annotations such as pathogenic, benign, likely pathogenic, and uncertain significance.

For this project, only **Single Nucleotide Polymorphisms (SNPs)** with clearly defined clinical labels (pathogenic and benign) were selected to ensure binary classification. Each variant entry includes the reference and alternate alleles along with the surrounding genomic sequence, which were used to construct the input features for the model.I have taken the Reference Sequences from the **Ensembl API**.

# Feature Engineering
To prepare the data for the model,I have applied a custom feature engineering pipeline.For each variant:
* A fixed-length reference sequence centered around the SNP position was extracted. 75 fron one side and 74 from other other side of the altered allele.
* The alternate sequence was generated by replacing the SNP location in the reference with the alternate allele.
* Both sequences were label encoded, mapping nucleotide characters (A, T, G, C) to integer values. This encoding converts the sequences into numerical format and preserves the positional imformation of the DNA bases.
* These encoded sequences served as dual inputs to the custom CNN-LSTM model, allowing it to learn both local patterns and long-range dependencies from the reference and alternate contexts.

This method allows the model to effectively capture the impact of the nucleotide change at the SNP position within its surrounding sequence environment.


# Model
I have read a [Research Paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC8285202/) with the title of **Analysis of DNA Sequence Classification Using CNN and Hybrid Models**.
I have read the resrach paper where they compared different CNN moeels for accuracy in classification tasks. 

After reading this I have come up with a new architecture of dual input CNN-LSTM structure to classify the DNA sequences.
I have label encoded the DNA bases (A,T,G,C) and the label encoding takinto account the positional imformation of the bases too. Then the 1D-CNN filters will extract features from the 150bp sequences that I have created through feature engineering and the LSTM layers will learn Long Term Dependencies of the sequence.

# Custom Interpretibility (Custom SmoothGrad)
To better understand what the model has learned and which base positions influence predictions, I implemented a custom version of the SmoothGrad interpretability technique.
SmoothGrad works by:
* Adding small amounts of noise to the input sequence multiple times
* Calculating the gradient of the model's output with respect to the input embedding for each noisy sample
* And averaging these gradients to obtain a smoothed saliency map highlighting which positions in the sequence the model finds most important.

In this project:
* I applied SmoothGrad to the output of the **Embedding layer**, using TensorFlow’s **GradientTape**.
* Separate saliency maps were generated for the *Reference* and *Alternate sequences*.
* The resulting heatmaps revesal which base positions contribute most to the model’s decision,particularly helping us check whether the model focuses on the alternate allele position.

![Importance Interpretibility](static/Screenshot%202025-07-26%20043026.png)
Here in the image we can see that the importance for **Alternate Sequence** is maximum at nearly 75th base position and I have added the alternate SNP allele there only during **feature engineering**. So it is learning to campture important features and focusing on the correct location.

# Precision-Recall Curve
I have used **Precision-Recall** Curve for **Threshold Tuning**, keeping in mind to balance the recall and preicison both sightly prioritising the Recall as False Negetives are more harmful than False Positives in case of Medical Machine Learning.

**I have specifically not used ROC for threshold tuning because in case of imbalanced test data the ROC can be slighly misleading because due to leage number of True Negtives the FPR will be very low even though the number of False Positive is very high. So slightly increasing the FPR can also increase the number of False Positives very much.**

![Precision vs Threshold Plot](static/Screenshot%202025-07-26%20160939.png)

In this plot we can see that the precision decreases with increase in threshold and we all can understand why because when we increase the threshold the **False Positive** decreases but also the **True Positive** decsreases and they does not chnge propotionately so it first increases then decreases.

![Precision vs Recall Curve](static/Screenshot%202025-07-26%20161000.png)

Here in the Precision vs Threshold curve we can see that the precision is maximum near the threshold of approx 0.7.

But we need to prioritise Recall more than Precision as **False Negetives** are more costly than **False Positives**

So I have plotted the Precision vs Recall curve and with the help of F1 score we can see that the best threshold for th best F1 score is 0.37.
So at the threshold the Precision and Recall is balanced as it has the highest F1 acotre among all other Precision and Recall combinations.

**And besides that we also have to increase the recall more so need to shift the threshod to somehwat less than 0.37 to increase the recall(as we can see fromt he recall vs threshold plot) but it also decreases preicison but we can excuse slight decrease in precision.**

![Recall vs Threshold](static/Screenshot%202025-07-26%20161023.png)

# Tech Stack
* Deep Learning: Tensorflow, Keras, Functional 
* Data Processing: pyfaidx, Numpy, Pandas, Ensembl API




