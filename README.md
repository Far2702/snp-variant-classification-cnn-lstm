# **Variant Classification of SNPs using a custom CNN-LSTM dual input architecture**
Project is not complete yet.
# Introduction
Single Nucleotide Polymorphisms (SNPs) are the most common type of genetic variation among individuals and play a crucial role in disease susceptibility, drug response, and personalized medicine. Accurate classification of SNPs as benign or pathogenic is essential for genomic diagnostics and therapeutic decision-making.

In this project, I have made a custom deep learning architecture that combines Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks to classify SNP variants. By leveraging dual input channels—one for reference sequences and another for alternate sequences, I aim to extract both local sequence motifs and long-range dependencies.

# Dataset
The dataset used for this project was obtained from **ClinVar**, a publicly available resource maintained by the **National Center for Biotechnology Information (NCBI)**. ClinVar aggregates information about genomic variations and their relationship to human health, including clinical significance annotations such as pathogenic, benign, likely pathogenic, and uncertain significance.

For this project, only **Single Nucleotide Polymorphisms (SNPs)** with clearly defined clinical labels (pathogenic and benign) were selected to ensure binary classification. Each variant entry includes the reference and alternate alleles along with the surrounding genomic sequence, which were used to construct the input features for the model.I have taken the Reference Sequences from the **Ensembl API**.

# Feature Engineering
To prepare the data for the model,I have applied a custom feature engineering pipeline.For each variant:
* A fixed-length reference sequence centered around the SNP position was extracted. 75 fron one side and 74 from other other side of the altered allele.
* The alternate sequence was generated by replacing the SNP location in the reference with the alternate allele.
* Both sequences were label encoded, mapping nucleotide characters (A, T, G, C) to integer values. This encoding converts the sequences into numerical format and preserves the positional imformation of the DNA bases.
* These encoded sequences served as dual inputs to the custom CNN-LSTM model, allowing it to learn both local patterns and long-range dependencies from the reference and alternate contexts.

This method allows the model to effectively capture the impact of the nucleotide change at the SNP position within its surrounding sequence environment.


# Model
I have read a [Research Paper](https://pmc.ncbi.nlm.nih.gov/articles/PMC8285202/) with the title of **Analysis of DNA Sequence Classification Using CNN and Hybrid Models**.
I have read the resrach paper where they compared different CNN moeels for accuracy in classification tasks. 

After reading this I have come up with a new architecture of dual input CNN-LSTM structure to classify the DNA sequences.
I have label encoded the DNA bases (A,T,G,C) and the label encoding takinto account the positional imformation of the bases too. Then the 1D-CNN filters will extract features from the 150bp sequences that I have created through feature engineering and the LSTM layers will learn Long Term Dependencies of the sequence.

# Custom Interpretibility (Custom SmoothGrad)
To better understand what the model has learned and which base positions influence predictions, I implemented a custom version of the SmoothGrad interpretability technique.
SmoothGrad works by:
* Adding small amounts of noise to the input sequence multiple times
* Calculating the gradient of the model's output with respect to the input embedding for each noisy sample
* And averaging these gradients to obtain a smoothed saliency map highlighting which positions in the sequence the model finds most important.

In this project:
* I applied SmoothGrad to the output of the **Embedding layer**, using TensorFlow’s **GradientTape**.
* Separate saliency maps were generated for the Reference and Alternate sequences.
* The resulting heatmaps reveal which base positions contribute most to the model’s decision,particularly helping us check whether the model focuses on the alternate allele position.

![Importance Interpretibility](static/Screenshot%202025-07-26%20043026.png)
Here in the image we can see that the importance for **Alternate Sequence** is maximum at nearly 75th base position and I have added the alternate SNP allele there only during **feature engineering**. So it is learning to campture important features and focusing on the correct location.

# Tech Stack
* Deep Learning: Tensorflow, Keras, Functional 
* Data Processing: pyfaidx, Numpy, Pandas, Ensembl API




